"""Web search module with multiple provider support."""

from ddgs import DDGS
from typing import List, Dict, Optional
from enum import Enum
import logging
import httpx
import os
import time
import asyncio
import yake

logger = logging.getLogger(__name__)

# YAKE keyword extractor configuration
_keyword_extractor: Optional[yake.KeywordExtractor] = None


def get_keyword_extractor() -> yake.KeywordExtractor:
    """Get or create YAKE keyword extractor (singleton for efficiency)."""
    global _keyword_extractor
    if _keyword_extractor is None:
        _keyword_extractor = yake.KeywordExtractor(
            lan="en",           # Language
            n=3,                # Max n-gram size (up to 3-word phrases)
            dedupLim=0.3,       # Stricter deduplication
            dedupFunc='seqm',   # Sequence matcher for dedup
            top=20,             # Extract more candidates, we'll filter
            features=None       # Use default features
        )
    return _keyword_extractor


# Noise words/phrases to filter out from extracted keywords
NOISE_WORDS = {
    # Action words from prompts
    'act', 'based', 'please', 'help', 'want', 'need', 'know', 'tell',
    'explain', 'describe', 'give', 'provide', 'show', 'make', 'create',
    # Analysis terms
    'question', 'answer', 'think', 'believe', 'consider', 'evaluate',
    'analyze', 'compare', 'discuss', 'strongest', 'arguments', 'theory',
    # Time/context noise
    'current', 'late', 'early', 'recent', 'today', 'now',
    # Common filler
    'like', 'using', 'use', 'way', 'things', 'something',
    # Prepositions/articles (YAKE sometimes includes these)
    'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'and', 'or'
}

# Phrases that should be filtered entirely
NOISE_PHRASES = {
    'market in late', 'analyst and evaluate', 'evaluate the theory',
    'compare the current', 'based on the', 'act as a', 'tell me about',
    'current market', 'late 2025', 'early 2025', 'in 2025', 'in 2024'
}

# Role-play job titles to filter (common in "act as a..." prompts)
ROLE_PLAY_TITLES = {
    'financial analyst', 'data analyst', 'business analyst', 'market analyst',
    'research analyst', 'investment analyst', 'senior analyst', 'junior analyst',
    'expert', 'specialist', 'consultant', 'advisor', 'professor', 'scientist',
    'economist', 'strategist', 'researcher', 'journalist', 'writer', 'editor'
}


def _preprocess_query(query: str) -> str:
    """
    Remove noise phrases and role-play titles from query BEFORE keyword extraction.
    This prevents YAKE from extracting words from these phrases.
    """
    import re
    cleaned = query

    # Remove role-play patterns like "act as a financial analyst"
    # This catches variations like "act as an expert", "acting as a consultant", etc.
    cleaned = re.sub(r'\b(act(ing)?|behave|pretend|imagine you are|you are|be) as (a|an|the)?\s*\w+(\s+\w+)?\b', '', cleaned, flags=re.IGNORECASE)

    # Remove specific role-play titles
    for title in ROLE_PLAY_TITLES:
        cleaned = re.sub(rf'\b{re.escape(title)}\b', '', cleaned, flags=re.IGNORECASE)

    # Remove noise phrases
    for phrase in NOISE_PHRASES:
        cleaned = re.sub(rf'\b{re.escape(phrase)}\b', '', cleaned, flags=re.IGNORECASE)

    # Clean up extra whitespace
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()

    return cleaned


def extract_search_keywords(query: str, max_keywords: int = 6) -> str:
    """
    Extract keywords from a user query using YAKE.
    Returns a space-separated string of keywords suitable for search engines.

    Args:
        query: The user's natural language query
        max_keywords: Maximum number of keywords to extract

    Returns:
        Optimized search query string
    """
    if not query or len(query.strip()) < 10:
        # Query too short, use as-is
        return query.strip()

    try:
        # Pre-process: Remove noise phrases and role-play titles BEFORE YAKE extraction
        cleaned_query = _preprocess_query(query)

        extractor = get_keyword_extractor()
        # YAKE returns list of (keyword, score) tuples, lower score = more important
        keywords = extractor.extract_keywords(cleaned_query)

        if not keywords:
            return query.strip()

        # Filter and clean keywords
        clean_keywords = []
        for kw, score in keywords:
            kw_lower = kw.lower()

            # Skip known noise phrases
            if kw_lower in NOISE_PHRASES:
                continue

            # Skip role-play job titles
            if kw_lower in ROLE_PLAY_TITLES:
                continue

            # Skip single-word noise
            words = kw_lower.split()
            if len(words) == 1 and words[0] in NOISE_WORDS:
                continue

            # Skip phrases where most words are noise
            non_noise_words = [w for w in words if w not in NOISE_WORDS]
            if len(non_noise_words) == 0:
                continue
            if len(words) > 1 and len(non_noise_words) < len(words) * 0.4:
                continue

            clean_keywords.append(kw)
            if len(clean_keywords) >= max_keywords:
                break

        # Remove keywords that are substrings of other keywords
        final_keywords = []
        for kw in clean_keywords:
            kw_lower = kw.lower()
            # Check if this keyword is a substring of any other keyword
            is_substring = False
            for other in clean_keywords:
                if kw != other and kw_lower in other.lower():
                    is_substring = True
                    break
            if not is_substring:
                final_keywords.append(kw)

        # Join into search query
        search_query = " ".join(final_keywords)

        logger.info(f"YAKE extracted keywords: '{search_query}' from query: '{query[:50]}...'")

        return search_query if search_query else query.strip()

    except Exception as e:
        logger.warning(f"YAKE keyword extraction failed: {e}, using original query")
        return query.strip()

# Rate limit handling
MAX_RETRIES = 2
RETRY_DELAY = 2  # seconds

# Total timeout budget for all search operations (including content fetching)
SEARCH_TIMEOUT_BUDGET = 60  # seconds total

# Persistent HTTP clients for connection pooling
_async_client: Optional[httpx.AsyncClient] = None
_sync_client: Optional[httpx.Client] = None


def get_async_client() -> httpx.AsyncClient:
    """Get or create persistent async HTTP client for connection pooling."""
    global _async_client
    if _async_client is None:
        _async_client = httpx.AsyncClient(timeout=30.0)
    return _async_client


def get_sync_client() -> httpx.Client:
    """Get or create persistent sync HTTP client for connection pooling."""
    global _sync_client
    if _sync_client is None:
        _sync_client = httpx.Client(timeout=30.0)
    return _sync_client


class SearchProvider(str, Enum):
    DUCKDUCKGO = "duckduckgo"
    TAVILY = "tavily"
    BRAVE = "brave"


async def perform_web_search(
    query: str,
    max_results: int = 5,
    provider: SearchProvider = SearchProvider.DUCKDUCKGO,
    full_content_results: int = 3,
    keyword_extraction: str = "direct"
) -> Dict[str, str]:
    """
    Perform a web search using the specified provider.

    Args:
        query: The search query
        max_results: Maximum number of results to return
        provider: Which search provider to use
        full_content_results: Number of top results to fetch full content for (0 to disable)
        keyword_extraction: "yake" for keyword extraction, "direct" for raw query

    Returns:
        Dict with 'results' (formatted string) and 'extracted_query' (keywords used)
    """
    # Extract keywords from user query if enabled, otherwise use direct query
    if keyword_extraction == "yake":
        extracted_query = extract_search_keywords(query)
    else:
        extracted_query = query.strip()

    try:
        if provider == SearchProvider.TAVILY:
            results = await _search_tavily(extracted_query, max_results)
        elif provider == SearchProvider.BRAVE:
            results = await _search_brave(extracted_query, max_results, full_content_results)
        else:
            # DuckDuckGo - now async with parallel Jina fetching (DDGS library calls are still sync but wrapped)
            results = await _search_duckduckgo(extracted_query, max_results, full_content_results)

        return {"results": results, "extracted_query": extracted_query}
    except Exception as e:
        logger.error(f"Error performing web search with {provider}: {str(e)}")
        return {
            "results": "[System Note: Web search was attempted but failed. Please answer based on your internal knowledge.]",
            "extracted_query": extracted_query
        }


async def _search_duckduckgo(query: str, max_results: int = 5, full_content_results: int = 3) -> str:
    """
    Search using DuckDuckGo (news search for better results).
    Optionally fetches full content via Jina Reader for top N results IN PARALLEL.
    """
    start_time = time.time()

    # Run the sync DDGS search in a thread to avoid blocking the event loop
    def _do_ddgs_search():
        """Sync helper for DDGS library which doesn't support async."""
        search_results_data = []
        urls_to_fetch = []

        for attempt in range(MAX_RETRIES + 1):
            try:
                with DDGS() as ddgs:
                    # Use text search (general web) instead of news for better coverage of facts/prices
                    search_results = list(ddgs.text(query, max_results=max_results))

                    for i, result in enumerate(search_results, 1):
                        title = result.get('title', 'No Title')
                        href = result.get('url', result.get('href', '#'))
                        body = result.get('body', result.get('excerpt', 'No description available.'))
                        source = result.get('source', '')

                        search_results_data.append({
                            'index': i,
                            'title': title,
                            'url': href,
                            'source': source,
                            'summary': body,
                            'content': None
                        })

                        # Queue top N results for full content fetch
                        if full_content_results > 0 and i <= full_content_results and href and href != '#':
                            urls_to_fetch.append((i - 1, href))
                    break  # Success, exit retry loop

            except Exception as e:
                if "Ratelimit" in str(e) and attempt < MAX_RETRIES:
                    logger.warning(f"DuckDuckGo rate limit hit, retrying in {RETRY_DELAY}s...")
                    time.sleep(RETRY_DELAY * (attempt + 1))
                else:
                    raise

        return search_results_data, urls_to_fetch

    # Execute sync DDGS search in thread pool
    search_results_data, urls_to_fetch = await asyncio.to_thread(_do_ddgs_search)

    # Fetch full content via Jina Reader for top results IN PARALLEL
    if urls_to_fetch:
        elapsed = time.time() - start_time
        remaining = SEARCH_TIMEOUT_BUDGET - elapsed

        if remaining > 5:  # Need at least 5s to fetch content
            # Calculate timeout for each fetch (use remaining time, capped at 25s)
            fetch_timeout = min(remaining, 25.0)

            # Create async tasks for all URLs
            async def fetch_with_index(idx: int, url: str):
                """Wrapper to return index along with content for result mapping."""
                content = await _fetch_with_jina(url, timeout=fetch_timeout)
                return (idx, content)

            tasks = [fetch_with_index(idx, url) for idx, url in urls_to_fetch]

            # Fetch all in parallel, handling individual failures gracefully
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Process results
            for result in results:
                if isinstance(result, Exception):
                    logger.warning(f"Parallel Jina fetch failed: {result}")
                    continue

                idx, content = result
                if content:
                    # If content is very short (likely paywall/cookie wall/failed parse),
                    # append the original summary to ensure we have some info.
                    if len(content) < 500:
                        original_summary = search_results_data[idx]['summary']
                        content += f"\n\n[System Note: Full content fetch yielded limited text. Appending original summary.]\nOriginal Summary: {original_summary}"
                    search_results_data[idx]['content'] = content
        else:
            logger.warning(f"Search timeout budget exhausted, skipping content fetches")

    if not search_results_data:
        return "No web search results found."

    # Format results
    formatted = []
    for r in search_results_data:
        text = f"Result {r['index']}:\nTitle: {r['title']}\nURL: {r['url']}"
        if r['source']:
            text += f"\nSource: {r['source']}"
        if r['content']:
            # Truncate content to ~2000 chars
            content = r['content'][:2000]
            if len(r['content']) > 2000:
                content += "..."
            text += f"\nContent:\n{content}"
        else:
            text += f"\nSummary: {r['summary']}"
        formatted.append(text)

    return "\n\n".join(formatted)


def _fetch_with_jina_sync(url: str, timeout: float = 25.0) -> Optional[str]:
    """
    Fetch article content using Jina Reader API (sync version for DuckDuckGo).
    Returns clean markdown content. Uses connection pooling.
    """
    try:
        jina_url = f"https://r.jina.ai/{url}"
        client = get_sync_client()
        response = client.get(jina_url, headers={
            "Accept": "text/plain",
        }, timeout=timeout)
        if response.status_code == 200:
            return response.text
        else:
            logger.warning(f"Jina Reader returned {response.status_code} for {url}")
            return None
    except httpx.TimeoutException:
        logger.warning(f"Timeout while fetching content via Jina for {url}")
        return None
    except Exception as e:
        logger.warning(f"Failed to fetch content via Jina for {url}: {e}")
        return None


async def _fetch_with_jina(url: str, timeout: float = 25.0) -> Optional[str]:
    """
    Fetch article content using Jina Reader API (async).
    Returns clean markdown content. Uses connection pooling.
    """
    try:
        jina_url = f"https://r.jina.ai/{url}"
        client = get_async_client()
        response = await client.get(jina_url, headers={
            "Accept": "text/plain",
        }, timeout=timeout)
        if response.status_code == 200:
            return response.text
        else:
            logger.warning(f"Jina Reader returned {response.status_code} for {url}")
            return None
    except httpx.TimeoutException:
        logger.warning(f"Timeout while fetching content via Jina for {url}")
        return None
    except Exception as e:
        logger.warning(f"Failed to fetch content via Jina for {url}: {e}")
        return None


async def _search_tavily(query: str, max_results: int = 5) -> str:
    """
    Search using Tavily API (designed for LLM/RAG use cases, async).
    Requires TAVILY_API_KEY environment variable. Uses connection pooling.
    """
    api_key = os.environ.get("TAVILY_API_KEY")
    if not api_key:
        logger.error("TAVILY_API_KEY not set")
        return "[System Note: Tavily API key not configured. Please add TAVILY_API_KEY to your environment.]"

    try:
        client = get_async_client()
        response = await client.post(
            "https://api.tavily.com/search",
            json={
                "api_key": api_key,
                "query": query,
                "max_results": max_results,
                "include_answer": False,
                "include_raw_content": False,
                "search_depth": "advanced",
            },
        )
        response.raise_for_status()
        data = response.json()

        results = []
        for i, result in enumerate(data.get("results", []), 1):
            title = result.get("title", "No Title")
            url = result.get("url", "#")
            content = result.get("content", "No content available.")

            text = f"Result {i}:\nTitle: {title}\nURL: {url}\nContent:\n{content}"
            results.append(text)

        if not results:
            return "No web search results found."

        return "\n\n".join(results)

    except httpx.HTTPStatusError as e:
        logger.error(f"Tavily API error: {e.response.status_code} - {e.response.text}")
        return "[System Note: Tavily search failed. Please check your API key.]"
    except Exception as e:
        logger.error(f"Tavily search error: {e}")
        return "[System Note: Tavily search failed. Please try again.]"


async def _search_brave(query: str, max_results: int = 5, full_content_results: int = 3) -> str:
    """
    Search using Brave Search API (async).
    Optionally fetches full content via Jina Reader for top N results.
    Requires BRAVE_API_KEY environment variable. Uses connection pooling.
    """
    start_time = time.time()
    api_key = os.environ.get("BRAVE_API_KEY")
    if not api_key:
        logger.error("BRAVE_API_KEY not set")
        return "[System Note: Brave API key not configured. Please add your Brave API key in settings.]"

    try:
        client = get_async_client()
        response = await client.get(
            "https://api.search.brave.com/res/v1/web/search",
            params={
                "q": query,
                "count": max_results,
            },
            headers={
                "Accept": "application/json",
                "X-Subscription-Token": api_key,
            },
        )
        response.raise_for_status()
        data = response.json()

        search_results_data = []
        urls_to_fetch = []
        web_results = data.get("web", {}).get("results", [])

        for i, result in enumerate(web_results[:max_results], 1):
            title = result.get("title", "No Title")
            url = result.get("url", "#")
            description = result.get("description", "No description available.")

            # Some results have extra_snippets with more content
            extra = result.get("extra_snippets", [])
            if extra:
                description += "\n" + "\n".join(extra[:2])

            search_results_data.append({
                'index': i,
                'title': title,
                'url': url,
                'summary': description,
                'content': None
            })

            # Queue top N results for full content fetch
            if full_content_results > 0 and i <= full_content_results and url and url != '#':
                urls_to_fetch.append((i - 1, url))

        # Fetch full content via Jina Reader for top results
        for idx, url in urls_to_fetch:
            # Check remaining time budget
            elapsed = time.time() - start_time
            remaining = SEARCH_TIMEOUT_BUDGET - elapsed

            if remaining <= 5:  # Need at least 5s to fetch content
                logger.warning(f"Search timeout budget exhausted, skipping remaining content fetches")
                break

            # Use remaining time as timeout for this fetch
            content = await _fetch_with_jina(url, timeout=min(remaining, 25.0))
            if content:
                # If content is very short, append summary
                if len(content) < 500:
                    original_summary = search_results_data[idx]['summary']
                    content += f"\n\n[System Note: Full content fetch yielded limited text. Appending original summary.]\nOriginal Summary: {original_summary}"
                search_results_data[idx]['content'] = content

        if not search_results_data:
            return "No web search results found."

        # Format results
        formatted = []
        for r in search_results_data:
            text = f"Result {r['index']}:\nTitle: {r['title']}\nURL: {r['url']}"
            if r['content']:
                # Truncate content to ~2000 chars
                content = r['content'][:2000]
                if len(r['content']) > 2000:
                    content += "..."
                text += f"\nContent:\n{content}"
            else:
                text += f"\nSummary: {r['summary']}"
            formatted.append(text)

        return "\n\n".join(formatted)

    except httpx.HTTPStatusError as e:
        logger.error(f"Brave API error: {e.response.status_code} - {e.response.text}")
        return "[System Note: Brave search failed. Please check your API key.]"
    except Exception as e:
        logger.error(f"Brave search error: {e}")
        return "[System Note: Brave search failed. Please try again.]"
